{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Jacobian matrix of a neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a simple neural network, with two affine layers followed by a ReLU and softmax non-linearities, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500  # Input size\n",
    "H = 100  # Hidden layer size\n",
    "O = 10   # Output size\n",
    "\n",
    "w1 = np.random.randn(N, H)\n",
    "b1 = np.random.randn(H)\n",
    "\n",
    "w2 = np.random.randn(H, O)\n",
    "b2 = np.random.randn(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Numpy implementation\n",
    "\"\"\"\n",
    "\n",
    "def ffpass_np(x):\n",
    "    a1 = np.dot(x, w1) + b1   # affine\n",
    "    r = np.maximum(0, a1)    # ReLU\n",
    "    a2 = np.dot(r, w2) + b2  # affine\n",
    "    \n",
    "    exps = np.exp(a2 - np.max(a2))  # softmax\n",
    "    out = exps / exps.sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the outputs would typically denote class propabilities; in which case some classification loss would be used for training.\n",
    "We do not focus on training here, but just on computing the Jacobian matrix of an existing neural network.\n",
    "\n",
    "Let's also write a Keras implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/julien/anaconda3/envs/jacobian/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Keras implementation\n",
    "\"\"\"\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(H, activation='relu', use_bias=True, input_dim=N))\n",
    "model.add(Dense(O, activation='softmax', use_bias=True, input_dim=O))\n",
    "model.get_layer(index=0).set_weights([w1, b1])\n",
    "model.get_layer(index=1).set_weights([w2, b2])\n",
    "    \n",
    "def ffpass_tf(x):\n",
    "    xr = x.reshape((1, x.size))\n",
    "    return model.predict(xr)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do our two implementations agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.random.random((N,))\n",
    "# x0 /= sum(x0)\n",
    "\n",
    "out_np = ffpass_np(x0)\n",
    "out_keras = ffpass_tf(x0)\n",
    "\n",
    "np.allclose(out_np, out_keras, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian matrix computation\n",
    "### Option 1: Tensorflow\n",
    "Unfortunately, tensorflow only provides computation of the gradient (with respect to a scalar output) -- see: https://github.com/tensorflow/tensorflow/issues/675.\n",
    "\n",
    "However, we can still iterate over each output, compute its gradient vector, and then group those vectors into a Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_tensorflow(x, verbose=False):    \n",
    "    jacobian_matrix = []\n",
    "    it = tqdm(range(O)) if verbose else range(O)\n",
    "    for o in it:\n",
    "        grad_func = tf.gradients(model.output[:, o], model.input)\n",
    "        gradients = sess.run(grad_func, feed_dict={model.input: x.reshape((1, x.size))})\n",
    "        jacobian_matrix.append(gradients[0][0,:])\n",
    "        \n",
    "    return np.array(jacobian_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.658 s. to compute the Jacobian matrix using Tensorflow\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "jacobian_tf = jacobian_tensorflow(x0, verbose=False)\n",
    "# %timeit jacobian_tf = jacobian_tensorflow(x0, verbose=False)\n",
    "tac = time.time()\n",
    "\n",
    "print('It took %.3f s. to compute the Jacobian matrix using Tensorflow' % (tac-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the Jacobian computation correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_jacobian_correct(jacobian_fn, ffpass_fn):\n",
    "    \"\"\" Numerical check of the Jacobian\n",
    "    \"\"\"\n",
    "    x = np.random.random((N,))\n",
    "    # x /= sum(x)\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    \"\"\" Check a few columns at random\n",
    "    \"\"\"\n",
    "    for idx in np.random.choice(N, 5, replace=False):\n",
    "    # for idx in range(5):\n",
    "        x2 = x.copy()\n",
    "        x2[idx] += epsilon\n",
    "\n",
    "        num_jacobian = (ffpass_fn(x2) - ffpass_fn(x)) / epsilon\n",
    "        computed_jacobian = jacobian_fn(x)\n",
    "        \n",
    "        if not all(abs(computed_jacobian[:, idx] - num_jacobian) < 1e-3):\n",
    "            \n",
    "            print('Found a mismatch.')\n",
    "            print('Numerical: {}'.format(num_jacobian[:10]))\n",
    "            print('Computed: {}'.format(computed_jacobian[:10, idx]))\n",
    "            \n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_jacobian_correct(jacobian_tensorflow, ffpass_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Autograd\n",
    "Autograd provides out-of-the-box automatic differentiation for Numpy-based functions.\n",
    "\n",
    "We have to start by re-defining our feedforward pass using the autograd's encapsulated Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as anp\n",
    "\n",
    "def ffpass_anp(x):\n",
    "    a1 = anp.dot(x, w1) + b1   # affine\n",
    "    a1 = anp.maximum(0, a1)    # ReLU\n",
    "    a2 = anp.dot(a1, w2) + b2  # affine\n",
    "    \n",
    "    exps = anp.exp(a2 - anp.max(a2))  # softmax\n",
    "    out = exps / exps.sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the output\n",
    "out_anp = ffpass_anp(x0)\n",
    "out_keras = ffpass_tf(x0)\n",
    "\n",
    "np.allclose(out_anp, out_keras, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Jacobian\n",
    "from autograd import jacobian\n",
    "\n",
    "def jacobian_autograd(x):\n",
    "    return jacobian(ffpass_anp)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is it correct?\n",
    "is_jacobian_correct(jacobian_autograd, ffpass_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.69 ms ± 135 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit jacobian_autograd(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do our two Jacobian matrices agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian_tf = jacobian_tensorflow(x0, verbose=False)\n",
    "jacobian_a = jacobian_autograd(x0)\n",
    "\n",
    "np.allclose(jacobian_tf, jacobian_a, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: write your backprop with Numpy\n",
    "Let's try to just re-implement our own backpropagation for this neural net. Here, I re-use the same kind of layer formalism as you can find in the http://cs231n.stanford.edu/ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Forward pass of an affine layer\n",
    "    :param x: input of dimension (D, )\n",
    "    :param w: weights matrix of dimension (D, M)\n",
    "    :param b: biais vector of dimension (M, )\n",
    "    :return output of dimension (M, ), and cache needed for backprop\n",
    "    \"\"\"\n",
    "    out = np.dot(x, w) + b\n",
    "    cache = (x, w)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for an affine layer.\n",
    "    :param dout: Upstream Jacobian, of shape (O, M)\n",
    "    :param cache: Tuple of:\n",
    "      - x: Input data, of shape (D, )\n",
    "      - w: Weights, of shape (D, M)\n",
    "    :return the jacobian matrix containing derivatives of the O neural network outputs with respect to\n",
    "            this layer's inputs, evaluated at x, of shape (O, D)\n",
    "    \"\"\"\n",
    "    x, w = cache\n",
    "    dx = np.dot(dout, w.T)\n",
    "    return dx\n",
    "\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\" Forward ReLU\n",
    "    \"\"\"\n",
    "    out = np.maximum(np.zeros(x.shape), x)\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass of ReLU\n",
    "    :param dout: Upstream Jacobian\n",
    "    :param cache: the cached input for this layer\n",
    "    :return: the jacobian matrix containing derivatives of the O neural network outputs with respect to\n",
    "             this layer's inputs, evaluated at x.\n",
    "    \"\"\"\n",
    "    x = cache\n",
    "    dx = dout * np.where(x > 0, np.ones(x.shape), np.zeros(x.shape))\n",
    "    return dx\n",
    "\n",
    "def softmax_forward(x):\n",
    "    \"\"\" Forward softmax\n",
    "    \"\"\"\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    s = exps / exps.sum()\n",
    "    return s, s\n",
    "    \n",
    "def softmax_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for softmax\n",
    "    :param dout: Upstream Jacobian\n",
    "    :param cache: contains the cache (in this case the output) for this layer\n",
    "    \"\"\"\n",
    "    s = cache\n",
    "    ds = np.diag(s) - np.outer(s, s.T)\n",
    "    dx = np.dot(dout, ds)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(x):\n",
    "    layer_to_cache = dict()  # for each layer, we store the cache needed for backward pass\n",
    "\n",
    "    # Forward pass:\n",
    "    a1, cache_a1 = affine_forward(x, w1, b1)\n",
    "    r1, cache_r1 = relu_forward(a1)\n",
    "    a2, cache_a2 = affine_forward(r1, w2, b2)\n",
    "    out, cache_out = softmax_forward(a2)\n",
    "\n",
    "    # backward pass\n",
    "    dout = np.diag(np.ones(out.size, ))  # the derivatives of each output w.r.t. each output.\n",
    "    dout = softmax_backward(dout, cache_out)\n",
    "    dout = affine_backward(dout, cache_a2)\n",
    "    dout = relu_backward(dout, cache_r1)\n",
    "    dx = affine_backward(dout, cache_a1)\n",
    "    \n",
    "    return out, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the output\n",
    "out_fb = forward_backward(x0)[0]\n",
    "out_keras = ffpass_tf(x0)\n",
    "\n",
    "np.allclose(out_fb, out_keras, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Jacobian\n",
    "is_jacobian_correct(lambda x: forward_backward(x)[1], ffpass_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 µs ± 2.38 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit forward_backward(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "For $N=500, H=100$ and $O=10$, I get about 650 ms for Tensorflow (granted, probably mine is not the best possible implementation), 3.5 ms with autograd, and 110 µs with Numpy.\n",
    "\n",
    "So for this problem, a home made Numpy implementation can be about 30 times faster than autograd, which is itself about 200 times faster than Tensorflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
